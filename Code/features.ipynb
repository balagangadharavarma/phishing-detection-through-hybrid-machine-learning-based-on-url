{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cl-KHPBKQcZl"
   },
   "source": [
    "### Having_ip_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dzlUVRqYQbu3"
   },
   "outputs": [],
   "source": [
    "def having_ip_address(url):\n",
    "  import re\n",
    "  x = re.search('^(http|https)://\\d+\\.\\d+\\.\\d+\\.\\d+\\.*', url)\n",
    "  if x:\n",
    "    return 1\n",
    "  else:\n",
    "    return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SKtLD4xYQ8N2",
    "outputId": "95031afa-665b-4234-8e83-6fade4872d7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "having_ip_address('https://128.36.54.192/questions/59020008/how-to-import-functions-of-a-jupyter-notebook-into-another-jupyter-notebook-in-g')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFiiEEkHT-cW"
   },
   "source": [
    "### URL Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "SLWmOZPeUDym"
   },
   "outputs": [],
   "source": [
    "def URL_Length(url):\n",
    "  if len(url) < 54:\n",
    "    return -1\n",
    "  elif len(url) >= 54 and len(url) <=75:\n",
    "    return 0\n",
    "  else:\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VgZRmWZYpxVV",
    "outputId": "109f2562-7d93-4264-893f-9a8944f9923b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URL_Length('https://stackoverflow.com/questions/59020008/how-to-import-functions-of-a-jupyter-notebook-into-another-jupyter-notebook-in-g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1xmDOAKWVRN"
   },
   "source": [
    "### Having @ Symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Q5G-TRuDUSrY"
   },
   "outputs": [],
   "source": [
    "def haveAtSign(url):\n",
    "  if \"@\" in url:\n",
    "    at = 1    \n",
    "  else:\n",
    "    at = -1    \n",
    "  return at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gghA3QASqFYD",
    "outputId": "403cfadd-b908-4c5e-9401-997c822e7b8e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "haveAtSign('https://stackoverflow.com/questions/59020008/how-to-import-functions-of-a-jupyter-notebook-into-another-jupyter-notebook-in-g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkegNyKBWZzW"
   },
   "source": [
    "### Prefix_Suffix  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VegEMYw-WOqO"
   },
   "outputs": [],
   "source": [
    "def prefixSuffix(url):\n",
    "    from urllib.parse import urlparse\n",
    "    if '-' in urlparse(url).netloc:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pqwOHhZUXkaO",
    "outputId": "11bb6fac-0409-4e2a-ce26-2f238b466763"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefixSuffix('https://dfhdfhdfgs.tokyo/ja-jp/account/login')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3T8WHUx1KW4d"
   },
   "source": [
    "### Having sub domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "tdRTQYEqKdmT"
   },
   "outputs": [],
   "source": [
    "def sub_domain_count(url):\n",
    "  !pip install tld\n",
    "  from urllib.parse import urlparse\n",
    "  from tld import get_tld\n",
    "  domain = urlparse(url).netloc\n",
    "  domain = domain.split('.')\n",
    "  top_domain = get_tld(url)\n",
    "  top_domain = top_domain.split('.')\n",
    "  sub_domain = set(domain) - set(top_domain)\n",
    "  count = len(sub_domain)\n",
    "  if(count == 1):\n",
    "    return -1\n",
    "  if(count == 2):\n",
    "    return 0\n",
    "  else:\n",
    "    return 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sNK0jN-1KijH",
    "outputId": "2cae26a9-27a4-4e89-81fa-542ea48c1a28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tld in c:\\users\\balag\\appdata\\roaming\\python\\python311\\site-packages (0.13)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_domain_count('https://stackoverflow.com/questions/59020008/how-to-import-functions-of-a-jupyter-notebook-into-another-jupyter-notebook-in-g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NAAy8DRHbl5"
   },
   "source": [
    "###SSL final state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6MF8WvH2HiSn"
   },
   "outputs": [],
   "source": [
    "def sslVerify(url):\n",
    "  from urllib.parse import urlparse,urlencode,urlsplit, urlunsplit\n",
    "  from urllib.request import Request, urlopen, ssl, socket\n",
    "  import json\n",
    "  from datetime import datetime\n",
    "  split_url = urlsplit(url)\n",
    "  #some site without http/https in the path\n",
    "  port = '443'\n",
    "\n",
    "  hostname = split_url.netloc\n",
    "  context = ssl.create_default_context()\n",
    "  try:\n",
    "    with socket.create_connection((hostname, port)) as sock:\n",
    "      with context.wrap_socket(sock, server_hostname=hostname) as ssock:\n",
    "          data = json.dumps(ssock.getpeercert())\n",
    "          res = json.loads(data)\n",
    "    notBefore = datetime.strptime(res[\"notBefore\"],'%b  %d %H:%M:%S %Y %Z').date()\n",
    "    notAfter = datetime.strptime(res[\"notAfter\"],'%b  %d %H:%M:%S %Y %Z').date()\n",
    "    # print(notBefore, notAfter)\n",
    "    if(ssl.SSLCertVerificationError(ssock) == True):\n",
    "      return 1\n",
    "    elif (notAfter.year-notBefore.year)+(notAfter.month-notBefore.month)*0.1 >= 1:\n",
    "      return -1\n",
    "    else:\n",
    "      return 0\n",
    "  except:\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DBOeXVpvH2lL",
    "outputId": "3928ba15-455c-4236-c35b-36cf45dfe007"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sslVerify(\"http://postdebanks.com/DIE/POST/diepost/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0m-feIvD6YF"
   },
   "source": [
    "### Port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "KPUhwNcED8x6"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def port(domain):\n",
    "    try:\n",
    "        response = requests.get(\"https://api.viewdns.info/portscan/?host=\"+domain+\"&apikey=1bf03196763a201bcc66a59bf88ed8ddf7a9432f&output=json\")\n",
    "        response.raise_for_status()  # Raise an exception for any HTTP errors\n",
    "\n",
    "        # Check if response contains valid JSON\n",
    "        try:\n",
    "            myjson = response.json()\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Failed to decode JSON response: {e}\")\n",
    "            return -1\n",
    "\n",
    "        pref_stat = {21: 'closed', 22: 'closed', 23: 'closed', 80: 'open', 443: 'open', 445: 'closed', 1433: 'closed', 1521: 'closed', 3306: 'closed', 3389: 'closed'}\n",
    "\n",
    "        flag = -1\n",
    "        for port_data in myjson.get('response', {}).get('port', []):\n",
    "            port_number = int(port_data.get('number', 0))\n",
    "            port_status = port_data.get('status', '')\n",
    "\n",
    "            if port_number in pref_stat and port_status != pref_stat[port_number]:\n",
    "                flag = 1\n",
    "                break  # Exit loop as soon as a port status doesn't match\n",
    "        return flag\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return -1  # Return -1 in case of any request exception\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "DcvJ8T_MEEws"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to decode JSON response: Expecting value: line 1 column 1 (char 0)\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "print(port('postdebanks.com'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SN1VuL2HsN5r"
   },
   "source": [
    "### Request_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "53pqH-W49_ss"
   },
   "outputs": [],
   "source": [
    "def request_url(url):\n",
    "  from urllib.parse import urlparse,urlencode,urlsplit, urlunsplit\n",
    "  from bs4 import BeautifulSoup\n",
    "  import requests\n",
    "  import re\n",
    "  !pip install tldextract\n",
    "  import tldextract\n",
    "  Null_format = [\"\", \"#\", \"#nothing\", \"#doesnotexist\", \"#null\", \"#void\", \"#whatever\",\n",
    "               \"#content\", \"javascript::void(0)\", \"javascript::void(0);\", \"javascript::;\", \"javascript\"]\n",
    "  \n",
    "  def is_URL_accessible(url):\n",
    "    page = None\n",
    "    try:\n",
    "        page = requests.get(url, timeout=5)   \n",
    "    except:\n",
    "        parsed = urlparse(url)\n",
    "        url = parsed.scheme+'://'+parsed.netloc\n",
    "        if not parsed.netloc.startswith('www'):\n",
    "            url = parsed.scheme+'://www.'+parsed.netloc\n",
    "            try:\n",
    "                page = requests.get(url, timeout=5)\n",
    "            except:\n",
    "                page = None\n",
    "                pass\n",
    "    if page and page.status_code == 200 and page.content not in [\"b''\", \"b' '\"]:\n",
    "        return True, url, page\n",
    "    else:\n",
    "        return False, None, None\n",
    "    \n",
    "  state, iurl, page = is_URL_accessible(url)\n",
    "\n",
    "  def get_domain(url):\n",
    "      o = urlsplit(url)\n",
    "      return o.hostname, tldextract.extract(url).domain, o.path\n",
    "    \n",
    "  if state:\n",
    "        print('Yes')\n",
    "        content = page.content\n",
    "        hostname, domain, path = get_domain(url)\n",
    "  else:\n",
    "        print('No state')\n",
    "  \n",
    "  Media = {'internals':[], 'externals':[], 'null':[]}\n",
    "\n",
    "  def external_media(Media):\n",
    "    total = len(Media['internals']) + len(Media['externals'])\n",
    "    externals = len(Media['externals'])\n",
    "    try:\n",
    "        percentile = externals / float(total) * 100\n",
    "    except:\n",
    "        return 0\n",
    "    return percentile\n",
    "  \n",
    "  def findMedia(Media,domain, hostname ):\n",
    "    Null_format = [\"\", \"#\", \"#nothing\", \"#doesnotexist\", \"#null\", \"#void\", \"#whatever\",\n",
    "               \"#content\", \"javascript::void(0)\", \"javascript::void(0);\", \"javascript::;\", \"javascript\"]\n",
    "    soup = BeautifulSoup(content, 'html.parser', from_encoding='iso-8859-1')\n",
    "    for img in soup.find_all('img', src=True):\n",
    "          dots = [x.start(0) for x in re.finditer('\\.', img['src'])]\n",
    "          if hostname in img['src'] or domain in img['src'] or len(dots) == 1 or not img['src'].startswith('http'):\n",
    "              if not img['src'].startswith('http'):\n",
    "                  if not img['src'].startswith('/'):\n",
    "                      Media['internals'].append(hostname+'/'+img['src']) \n",
    "                  elif img['src'] in Null_format:\n",
    "                      Media['null'].append(img['src'])  \n",
    "                  else:\n",
    "                      Media['internals'].append(hostname+img['src'])   \n",
    "          else:\n",
    "              Media['externals'].append(img['src'])\n",
    "    for audio in soup.find_all('audio', src=True):\n",
    "      dots = [x.start(0) for x in re.finditer('\\.', audio['src'])]\n",
    "      if hostname in audio['src'] or domain in audio['src'] or len(dots) == 1 or not audio['src'].startswith('http'):\n",
    "          if not audio['src'].startswith('http'):\n",
    "              if not audio['src'].startswith('/'):\n",
    "                  Media['internals'].append(hostname+'/'+audio['src']) \n",
    "              elif audio['src'] in Null_format:\n",
    "                  Media['null'].append(audio['src'])  \n",
    "              else:\n",
    "                  Media['internals'].append(hostname+audio['src'])   \n",
    "      else:\n",
    "          Media['externals'].append(audio['src'])\n",
    "          \n",
    "    for embed in soup.find_all('embed', src=True):\n",
    "      dots = [x.start(0) for x in re.finditer('\\.', embed['src'])]\n",
    "      if hostname in embed['src'] or domain in embed['src'] or len(dots) == 1 or not embed['src'].startswith('http'):\n",
    "          if not embed['src'].startswith('http'):\n",
    "              if not embed['src'].startswith('/'):\n",
    "                  Media['internals'].append(hostname+'/'+embed['src']) \n",
    "              elif embed['src'] in Null_format:\n",
    "                  Media['null'].append(embed['src'])  \n",
    "              else:\n",
    "                  Media['internals'].append(hostname+embed['src'])   \n",
    "      else:\n",
    "          Media['externals'].append(embed['src'])\n",
    "        \n",
    "    for i_frame in soup.find_all('iframe', src=True):\n",
    "      dots = [x.start(0) for x in re.finditer('\\.', i_frame['src'])]\n",
    "      if hostname in i_frame['src'] or domain in i_frame['src'] or len(dots) == 1 or not i_frame['src'].startswith('http'):\n",
    "          if not i_frame['src'].startswith('http'):\n",
    "              if not i_frame['src'].startswith('/'):\n",
    "                  Media['internals'].append(hostname+'/'+i_frame['src']) \n",
    "              elif i_frame['src'] in Null_format:\n",
    "                  Media['null'].append(i_frame['src'])  \n",
    "              else:\n",
    "                  Media['internals'].append(hostname+i_frame['src'])   \n",
    "      else: \n",
    "          Media['externals'].append(i_frame['src'])\n",
    "  \n",
    "  findMedia(Media, domain, hostname)\n",
    "  if external_media(Media) < 22:\n",
    "    return -1\n",
    "  elif external_media(Media) >= 22 and external_media(Media) < 61:\n",
    "    return 0\n",
    "  else:\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "D03EVESN_Q18"
   },
   "outputs": [],
   "source": [
    "# request_url('http://www.budgetbots.com/server.php/Server%20update/index.php?email=USER@DOMAIN.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kZqNCVNbI8n"
   },
   "source": [
    "### Safe anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "VXhH39Y2slwN"
   },
   "outputs": [],
   "source": [
    "def url_anchor(url):\n",
    "  from urllib.parse import urlparse,urlencode,urlsplit, urlunsplit\n",
    "  from bs4 import BeautifulSoup\n",
    "  import requests\n",
    "  import re\n",
    "  !pip install tldextract\n",
    "  import tldextract\n",
    "  Null_format = [\"\", \"#\", \"#nothing\", \"#doesnotexist\", \"#null\", \"#void\", \"#whatever\",\n",
    "               \"#content\", \"javascript::void(0)\", \"javascript::void(0);\", \"javascript::;\", \"javascript\"]\n",
    "  \n",
    "  def is_URL_accessible(url):\n",
    "    page = None\n",
    "    try:\n",
    "        page = requests.get(url, timeout=5)   \n",
    "    except:\n",
    "        parsed = urlparse(url)\n",
    "        url = parsed.scheme+'://'+parsed.netloc\n",
    "        if not parsed.netloc.startswith('www'):\n",
    "            url = parsed.scheme+'://www.'+parsed.netloc\n",
    "            try:\n",
    "                page = requests.get(url, timeout=5)\n",
    "            except:\n",
    "                page = None\n",
    "                pass\n",
    "    if page and page.status_code == 200 and page.content not in [\"b''\", \"b' '\"]:\n",
    "        return True, url, page\n",
    "    else:\n",
    "        return False, None, None\n",
    "    \n",
    "  state, iurl, page = is_URL_accessible(url)\n",
    "\n",
    "  def get_domain(url):\n",
    "      o = urlsplit(url)\n",
    "      return o.hostname, tldextract.extract(url).domain, o.path\n",
    "    \n",
    "  if state:\n",
    "        content = page.content\n",
    "        hostname, domain, path = get_domain(url)\n",
    "    \n",
    "  Anchor = {'safe':[], 'unsafe':[], 'null':[]}\n",
    "\n",
    "  def anchor(Anchor, domain, hostname):\n",
    "      soup = BeautifulSoup(content, 'html.parser', from_encoding='iso-8859-1')\n",
    "      for href in soup.find_all('a', href=True):\n",
    "        dots = [x.start(0) for x in re.finditer('\\.', href['href'])]\n",
    "        if hostname in href['href'] or domain in href['href'] or len(dots) == 1 or not href['href'].startswith('http'):\n",
    "            if \"#\" in href['href'] or \"javascript\" in href['href'].lower() or \"mailto\" in href['href'].lower():\n",
    "                 Anchor['unsafe'].append(href['href'])\n",
    "        else:\n",
    "            Anchor['safe'].append(href['href'])\n",
    "\n",
    "  anchor(Anchor, domain, url)\n",
    "\n",
    "  def safe_anchor(Anchor):\n",
    "      total = len(Anchor['safe']) +  len(Anchor['unsafe'])\n",
    "      unsafe = len(Anchor['unsafe'])\n",
    "      try:\n",
    "        percentile = unsafe / float(total) * 100\n",
    "      except:\n",
    "        return 0\n",
    "      return percentile\n",
    "  #print(safe_anchor(Anchor))   \n",
    "  if safe_anchor(Anchor) < 31:\n",
    "       return -1\n",
    "  elif safe_anchor(Anchor) >= 31 and safe_anchor(Anchor) <= 67:\n",
    "       return 0\n",
    "  else:\n",
    "       return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J3T8ZDedxchB",
    "outputId": "c14f39e1-9cfe-4057-a114-ed0374892cef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tldextract in c:\\installation folder\\anaconda\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: idna in c:\\users\\balag\\appdata\\roaming\\python\\python311\\site-packages (from tldextract) (2.10)\n",
      "Requirement already satisfied: requests>=2.1.0 in c:\\users\\balag\\appdata\\roaming\\python\\python311\\site-packages (from tldextract) (2.25.1)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\installation folder\\anaconda\\lib\\site-packages (from tldextract) (1.5.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\installation folder\\anaconda\\lib\\site-packages (from tldextract) (3.13.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\installation folder\\anaconda\\lib\\site-packages (from requests>=2.1.0->tldextract) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\installation folder\\anaconda\\lib\\site-packages (from requests>=2.1.0->tldextract) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\installation folder\\anaconda\\lib\\site-packages (from requests>=2.1.0->tldextract) (2024.2.2)\n",
      "Requirement already satisfied: six in c:\\installation folder\\anaconda\\lib\\site-packages (from requests-file>=1.4->tldextract) (1.16.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_anchor('https://www.xiaoji.com/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2ChpOldzG4c"
   },
   "source": [
    "### Links in Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "qyEfGEeUzePr"
   },
   "outputs": [],
   "source": [
    "def links_tag(url):\n",
    "  from urllib.parse import urlparse,urlencode,urlsplit, urlunsplit\n",
    "  from bs4 import BeautifulSoup\n",
    "  import requests\n",
    "  import re\n",
    "  !pip install tldextract\n",
    "  import tldextract\n",
    "  Null_format = [\"\", \"#\", \"#nothing\", \"#doesnotexist\", \"#null\", \"#void\", \"#whatever\",\n",
    "               \"#content\", \"javascript::void(0)\", \"javascript::void(0);\", \"javascript::;\", \"javascript\"]\n",
    "  \n",
    "  def is_URL_accessible(url):\n",
    "    page = None\n",
    "    try:\n",
    "        page = requests.get(url, timeout=5)   \n",
    "    except:\n",
    "        parsed = urlparse(url)\n",
    "        url = parsed.scheme+'://'+parsed.netloc\n",
    "        if not parsed.netloc.startswith('www'):\n",
    "            url = parsed.scheme+'://www.'+parsed.netloc\n",
    "            try:\n",
    "                page = requests.get(url, timeout=5)\n",
    "            except:\n",
    "                page = None\n",
    "                pass\n",
    "    if page and page.status_code == 200 and page.content not in [\"b''\", \"b' '\"]:\n",
    "        return True, url, page\n",
    "    else:\n",
    "        return False, None, None\n",
    "    \n",
    "  state, iurl, page = is_URL_accessible(url)\n",
    "\n",
    "  def get_domain(url):\n",
    "      o = urlsplit(url)\n",
    "      return o.hostname, tldextract.extract(url).domain, o.path\n",
    "    \n",
    "  if state:\n",
    "        content = page.content\n",
    "        hostname, domain, path = get_domain(url)\n",
    "  \n",
    "  Link = {'internals':[], 'externals':[], 'null':[]}\n",
    "\n",
    "  def find_links(Link, domain, hostname):\n",
    "    soup = BeautifulSoup(content, 'html.parser', from_encoding='iso-8859-1')\n",
    "    for link in soup.findAll('link', href=True):\n",
    "        dots = [x.start(0) for x in re.finditer('\\.', link['href'])]\n",
    "        if hostname in link['href'] or domain in link['href'] or len(dots) == 1 or not link['href'].startswith('http'):\n",
    "            if not link['href'].startswith('http'):\n",
    "                if not link['href'].startswith('/'):\n",
    "                    Link['internals'].append(hostname+'/'+link['href']) \n",
    "                elif link['href'] in Null_format:\n",
    "                    Link['null'].append(link['href'])  \n",
    "                else:\n",
    "                    Link['internals'].append(hostname+link['href'])   \n",
    "        else:\n",
    "            Link['externals'].append(link['href'])\n",
    "\n",
    "    for script in soup.find_all('script', src=True):\n",
    "        dots = [x.start(0) for x in re.finditer('\\.', script['src'])]\n",
    "        if hostname in script['src'] or domain in script['src'] or len(dots) == 1 or not script['src'].startswith('http'):\n",
    "            if not script['src'].startswith('http'):\n",
    "                if not script['src'].startswith('/'):\n",
    "                    Link['internals'].append(hostname+'/'+script['src']) \n",
    "                elif script['src'] in Null_format:\n",
    "                    Link['null'].append(script['src'])  \n",
    "                else:\n",
    "                    Link['internals'].append(hostname+script['src'])   \n",
    "        else:\n",
    "            Link['externals'].append(link['href'])\n",
    "  \n",
    "  def links_in_tags(Link):\n",
    "    total = len(Link['internals']) +  len(Link['externals'])\n",
    "    internals = len(Link['internals'])\n",
    "    try:\n",
    "        percentile = internals / float(total) * 100\n",
    "    except:\n",
    "        return 0\n",
    "    return percentile\n",
    "  #print(links_in_tags(Link))\n",
    "  if links_in_tags(Link) < 17:\n",
    "    return -1\n",
    "  elif links_in_tags(Link) >= 17 and links_in_tags(Link) <= 81:\n",
    "    return 0\n",
    "  else:\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OuyX7gou0lEF",
    "outputId": "949dfefb-ca39-4e9d-f76f-275e17fc9bdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tldextract in c:\\installation folder\\anaconda\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: idna in c:\\users\\balag\\appdata\\roaming\\python\\python311\\site-packages (from tldextract) (2.10)\n",
      "Requirement already satisfied: requests>=2.1.0 in c:\\users\\balag\\appdata\\roaming\\python\\python311\\site-packages (from tldextract) (2.25.1)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\installation folder\\anaconda\\lib\\site-packages (from tldextract) (1.5.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\installation folder\\anaconda\\lib\\site-packages (from tldextract) (3.13.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\installation folder\\anaconda\\lib\\site-packages (from requests>=2.1.0->tldextract) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\installation folder\\anaconda\\lib\\site-packages (from requests>=2.1.0->tldextract) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\installation folder\\anaconda\\lib\\site-packages (from requests>=2.1.0->tldextract) (2024.2.2)\n",
      "Requirement already satisfied: six in c:\\installation folder\\anaconda\\lib\\site-packages (from requests-file>=1.4->tldextract) (1.16.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links_tag('https://www.xiaoji.com/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFbL5g40AHXG"
   },
   "source": [
    "###SFH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "8K3HAagUAJiq"
   },
   "outputs": [],
   "source": [
    "def sfh(url):\n",
    "  from urllib.parse import urlparse,urlencode,urlsplit, urlunsplit\n",
    "  from bs4 import BeautifulSoup\n",
    "  import requests\n",
    "  import re\n",
    "  !pip install tldextract\n",
    "  import tldextract\n",
    "  Null_format = [\"\", \"#\", \"#nothing\", \"#doesnotexist\", \"#null\", \"#void\", \"#whatever\",\n",
    "               \"#content\", \"javascript::void(0)\", \"javascript::void(0);\", \"javascript::;\", \"javascript\"]\n",
    "  \n",
    "  def is_URL_accessible(url):\n",
    "    page = None\n",
    "    try:\n",
    "        page = requests.get(url, timeout=5)   \n",
    "    except:\n",
    "        parsed = urlparse(url)\n",
    "        url = parsed.scheme+'://'+parsed.netloc\n",
    "        if not parsed.netloc.startswith('www'):\n",
    "            url = parsed.scheme+'://www.'+parsed.netloc\n",
    "            try:\n",
    "                page = requests.get(url, timeout=5)\n",
    "            except:\n",
    "                page = None\n",
    "                pass\n",
    "    if page and page.status_code == 200 and page.content not in [\"b''\", \"b' '\"]:\n",
    "        return True, url, page\n",
    "    else:\n",
    "        return False, None, None\n",
    "    \n",
    "  state, iurl, page = is_URL_accessible(url)\n",
    "\n",
    "  def get_domain(url):\n",
    "      o = urlsplit(url)\n",
    "      return o.hostname, tldextract.extract(url).domain, o.path\n",
    "    \n",
    "  if state:\n",
    "        content = page.content\n",
    "        hostname, domain, path = get_domain(url)\n",
    "  \n",
    "  Form = {'internals':[], 'externals':[], 'null':[]}\n",
    "\n",
    "  def findForm(Form, domain, hostname):\n",
    "    for form in soup.findAll('form', action=True):\n",
    "      dots = [x.start(0) for x in re.finditer('\\.', form['action'])]\n",
    "      if hostname in form['action'] or domain in form['action'] or len(dots) == 1 or not form['action'].startswith('http'):\n",
    "          if not form['action'].startswith('http'):\n",
    "              if not form['action'].startswith('/'):\n",
    "                  Form['internals'].append(hostname+'/'+form['action']) \n",
    "              elif form['action'] in Null_format or form['action'] == 'about:blank':\n",
    "                  Form['null'].append(form['action'])  \n",
    "              else:\n",
    "                  Form['internals'].append(hostname+form['action'])   \n",
    "      else:\n",
    "          Form['externals'].append(form['action'])\n",
    "  \n",
    "  def sf(hostname, Form):\n",
    "    if len(Form['null'])==0:\n",
    "        return 1\n",
    "    elif len(Form['null'])>0:\n",
    "        return 0\n",
    "    else:\n",
    "      return -1\n",
    "  \n",
    "  return(sf(hostname, Form))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-tQjn-8NAZK-",
    "outputId": "e7e88e82-dc8f-45e6-87d5-32ee15d950ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tldextract in c:\\installation folder\\anaconda\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: idna in c:\\users\\balag\\appdata\\roaming\\python\\python311\\site-packages (from tldextract) (2.10)\n",
      "Requirement already satisfied: requests>=2.1.0 in c:\\users\\balag\\appdata\\roaming\\python\\python311\\site-packages (from tldextract) (2.25.1)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\installation folder\\anaconda\\lib\\site-packages (from tldextract) (1.5.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\installation folder\\anaconda\\lib\\site-packages (from tldextract) (3.13.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\installation folder\\anaconda\\lib\\site-packages (from requests>=2.1.0->tldextract) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\installation folder\\anaconda\\lib\\site-packages (from requests>=2.1.0->tldextract) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\installation folder\\anaconda\\lib\\site-packages (from requests>=2.1.0->tldextract) (2024.2.2)\n",
      "Requirement already satisfied: six in c:\\installation folder\\anaconda\\lib\\site-packages (from requests-file>=1.4->tldextract) (1.16.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfh('https://stackoverflow.com/questions/59020008/how-to-import-functions-of-a-jupyter-notebook-into-another-jupyter-notebook-in-g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9T17m9VotY_"
   },
   "source": [
    "### Submitting to emial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "bRQcMGYr1V5L"
   },
   "outputs": [],
   "source": [
    "def sub_email(url):\n",
    "  from urllib.parse import urlparse,urlencode,urlsplit, urlunsplit\n",
    "  from bs4 import BeautifulSoup\n",
    "  import requests\n",
    "  import re\n",
    "  !pip install tldextract\n",
    "  import tldextract\n",
    "  Null_format = [\"\", \"#\", \"#nothing\", \"#doesnotexist\", \"#null\", \"#void\", \"#whatever\",\n",
    "               \"#content\", \"javascript::void(0)\", \"javascript::void(0);\", \"javascript::;\", \"javascript\"]\n",
    "  \n",
    "  def is_URL_accessible(url):\n",
    "    page = None\n",
    "    try:\n",
    "        page = requests.get(url, timeout=5)   \n",
    "    except:\n",
    "        parsed = urlparse(url)\n",
    "        url = parsed.scheme+'://'+parsed.netloc\n",
    "        if not parsed.netloc.startswith('www'):\n",
    "            url = parsed.scheme+'://www.'+parsed.netloc\n",
    "            try:\n",
    "                page = requests.get(url, timeout=5)\n",
    "            except:\n",
    "                page = None\n",
    "                pass\n",
    "    if page and page.status_code == 200 and page.content not in [\"b''\", \"b' '\"]:\n",
    "        return True, url, page\n",
    "    else:\n",
    "        return False, None, None\n",
    "    \n",
    "  state, iurl, page = is_URL_accessible(url)\n",
    "\n",
    "  def get_domain(url):\n",
    "      o = urlsplit(url)\n",
    "      return o.hostname, tldextract.extract(url).domain, o.path\n",
    "    \n",
    "  if state:\n",
    "        content = page.content\n",
    "        hostname, domain, path = get_domain(url)\n",
    "  \n",
    "  Form = {'internals':[], 'externals':[], 'null':[]}\n",
    "  def findForm(Form, domain, hostname):\n",
    "    for form in soup.findAll('form', action=True):\n",
    "      dots = [x.start(0) for x in re.finditer('\\.', form['action'])]\n",
    "      if hostname in form['action'] or domain in form['action'] or len(dots) == 1 or not form['action'].startswith('http'):\n",
    "          if not form['action'].startswith('http'):\n",
    "              if not form['action'].startswith('/'):\n",
    "                  Form['internals'].append(hostname+'/'+form['action']) \n",
    "              elif form['action'] in Null_format or form['action'] == 'about:blank':\n",
    "                  Form['null'].append(form['action'])  \n",
    "              else:\n",
    "                  Form['internals'].append(hostname+form['action'])   \n",
    "      else:\n",
    "          Form['externals'].append(form['action'])\n",
    "  \n",
    "  def submitting_to_email(Form):\n",
    "    # print(Form)\n",
    "    for form in (Form['internals'] + Form['externals']):\n",
    "        #print('inside for')\n",
    "        if \"mailto:\" in form or \"mail()\" in form:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    return 1\n",
    "  \n",
    "  return(submitting_to_email(Form))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "gvn4GtcK2Ygi"
   },
   "outputs": [],
   "source": [
    "# print('ans is ',  sub_email('https://www.xiaoji.com/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmdyOA2u9J9U"
   },
   "source": [
    "### On Mouse-over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "SSjdgTX49VKn"
   },
   "outputs": [],
   "source": [
    "def mouse_over(url):\n",
    "  from urllib.parse import urlparse,urlencode,urlsplit, urlunsplit\n",
    "  from bs4 import BeautifulSoup\n",
    "  import requests\n",
    "  import re\n",
    "  !pip install tldextract\n",
    "  import tldextract\n",
    "  Null_format = [\"\", \"#\", \"#nothing\", \"#doesnotexist\", \"#null\", \"#void\", \"#whatever\",\n",
    "               \"#content\", \"javascript::void(0)\", \"javascript::void(0);\", \"javascript::;\", \"javascript\"]\n",
    "  \n",
    "  def is_URL_accessible(url):\n",
    "    page = None\n",
    "    try:\n",
    "        page = requests.get(url, timeout=5)   \n",
    "    except:\n",
    "        parsed = urlparse(url)\n",
    "        url = parsed.scheme+'://'+parsed.netloc\n",
    "        if not parsed.netloc.startswith('www'):\n",
    "            url = parsed.scheme+'://www.'+parsed.netloc\n",
    "            try:\n",
    "                page = requests.get(url, timeout=5)\n",
    "            except:\n",
    "                page = None\n",
    "                pass\n",
    "    if page and page.status_code == 200 and page.content not in [\"b''\", \"b' '\"]:\n",
    "        return True, url, page\n",
    "    else:\n",
    "        return False, None, None\n",
    "    \n",
    "  state, iurl, page = is_URL_accessible(url)\n",
    "\n",
    "  def get_domain(url):\n",
    "      o = urlsplit(url)\n",
    "      return o.hostname, tldextract.extract(url).domain, o.path\n",
    "    \n",
    "  if state:\n",
    "        content = page.content\n",
    "        hostname, domain, path = get_domain(url)\n",
    "  \n",
    "  def onmouseover(content):\n",
    "    if 'onmouseover=\"window.status=' in str(content).lower().replace(\" \",\"\"):\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "  return(onmouseover(content.decode('latin-1')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RIVx7oKD9i4R",
    "outputId": "eec02752-37de-4101-f6e6-da87f73be87b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tldextract in c:\\installation folder\\anaconda\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: idna in c:\\users\\balag\\appdata\\roaming\\python\\python311\\site-packages (from tldextract) (2.10)\n",
      "Requirement already satisfied: requests>=2.1.0 in c:\\users\\balag\\appdata\\roaming\\python\\python311\\site-packages (from tldextract) (2.25.1)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\installation folder\\anaconda\\lib\\site-packages (from tldextract) (1.5.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\installation folder\\anaconda\\lib\\site-packages (from tldextract) (3.13.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\installation folder\\anaconda\\lib\\site-packages (from requests>=2.1.0->tldextract) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\installation folder\\anaconda\\lib\\site-packages (from requests>=2.1.0->tldextract) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\installation folder\\anaconda\\lib\\site-packages (from requests>=2.1.0->tldextract) (2024.2.2)\n",
      "Requirement already satisfied: six in c:\\installation folder\\anaconda\\lib\\site-packages (from requests-file>=1.4->tldextract) (1.16.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mouse_over('https://www.xiaoji.com/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fsn8SE8Z7a8r"
   },
   "source": [
    "### Right Click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "Qsfv_qCv7d_v"
   },
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, urlsplit\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import tldextract\n",
    "\n",
    "def right_click(url):\n",
    "    Null_format = [\"\", \"#\", \"#nothing\", \"#doesnotexist\", \"#null\", \"#void\", \"#whatever\",\n",
    "                   \"#content\", \"javascript::void(0)\", \"javascript::void(0);\", \"javascript::;\", \"javascript\"]\n",
    "\n",
    "    def is_URL_accessible(url):\n",
    "        page = None\n",
    "        try:\n",
    "            page = requests.get(url, timeout=5)   \n",
    "        except:\n",
    "            parsed = urlparse(url)\n",
    "            url = parsed.scheme+'://'+parsed.netloc\n",
    "            if not parsed.netloc.startswith('www'):\n",
    "                url = parsed.scheme+'://www.'+parsed.netloc\n",
    "                try:\n",
    "                    page = requests.get(url, timeout=5)\n",
    "                except:\n",
    "                    page = None\n",
    "                    pass\n",
    "        if page and page.status_code == 200 and page.content not in [\"b''\", \"b' '\"]:\n",
    "            return True, url, page\n",
    "        else:\n",
    "            return False, None, None\n",
    "\n",
    "    state, iurl, page = is_URL_accessible(url)\n",
    "\n",
    "    if state:\n",
    "        content = page.content\n",
    "        \n",
    "        def get_domain(url):\n",
    "            o = urlsplit(url)\n",
    "            return o.hostname, tldextract.extract(url).domain, o.path\n",
    "\n",
    "        hostname, domain, path = get_domain(url)\n",
    "\n",
    "        def right_clic(content):\n",
    "            if re.findall(r\"event.button ?== ?2\", content.decode('latin-1')):\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "\n",
    "        return right_clic(content)\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xOK7rSVZ7vhG",
    "outputId": "a1ffd1af-e5c5-43e9-b8d1-f856c392e393"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(right_click('http://walletconnectbits.com/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPr_9zxxPK8m"
   },
   "source": [
    "Domain Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "txf3wxJLPLX-"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "def domain_age(domain):\n",
    "    url = domain.split(\"//\")[-1].split(\"/\")[0].split('?')[0]\n",
    "    show = \"https://input.payapi.io/v1/api/fraud/domain/age/\" + url\n",
    "\n",
    "    retries = 3  # Number of retries\n",
    "    timeout = 10  # Timeout in seconds\n",
    "\n",
    "    for _ in range(retries):\n",
    "        try:\n",
    "            r = requests.get(show, timeout=timeout)\n",
    "            if r.status_code == 200:\n",
    "                data = r.text\n",
    "                jsonToPython = json.loads(data)\n",
    "                result = jsonToPython['result']\n",
    "                if result / 30 >= 6:\n",
    "                    return -1\n",
    "                else:\n",
    "                    return 1\n",
    "            else:\n",
    "                return -1\n",
    "        except requests.exceptions.Timeout:\n",
    "            # Retry on timeout\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {e}\")\n",
    "            return -1\n",
    "\n",
    "    # Return -1 if all retries fail\n",
    "    return -1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QOHwDfCvPY9N",
    "outputId": "9e965d1b-1001-43fd-a970-9383619be6d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    }
   ],
   "source": [
    "print(domain_age('http://walletconnectbits.com/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uG7HHBmbloIs"
   },
   "source": [
    "### DNS Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "e_K7UyfSlh5k"
   },
   "outputs": [],
   "source": [
    "def dns_record(domain):\n",
    "    !pip install dnspython\n",
    "    import dns.resolver\n",
    "    try:\n",
    "        nameservers = dns.resolver.resolve(domain,'NS')\n",
    "        if len(nameservers) > 0:\n",
    "            #print('len is', nameservers)\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "    except:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "Yu5ITC0wmPTl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dnspython in c:\\installation folder\\anaconda\\lib\\site-packages (2.6.1)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(dns_record('http://postdebanks.com/DIE/POST/diepost/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xS_bgn_U65Rz"
   },
   "source": [
    "###Website Traffic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "xMhbbEtg3wkR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import quote\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.error import URLError\n",
    "\n",
    "def web_traffic(url):\n",
    "    try:\n",
    "        # Filling the whitespaces in the URL if any\n",
    "        url = quote(url)\n",
    "        rank = BeautifulSoup(urlopen(\"http://data.alexa.com/data?cli=10&dat=s&url=\" + url).read(), \"xml\").find(\"REACH\")['RANK']\n",
    "        rank = int(rank)\n",
    "        if rank <= 100000:\n",
    "            return -1\n",
    "        elif rank > 100000:\n",
    "            return 0\n",
    "    except (TypeError, KeyError, URLError):\n",
    "        # Handle exceptions gracefully\n",
    "        return 1\n",
    "\n",
    "# Test the function\n",
    "print(web_traffic('http://postdebanks.com/DIE/POST/diepost/'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "RxqEciG03zj8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "print(web_traffic('http://postdebanks.com/DIE/POST/diepost/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OCb96p6XDn3R"
   },
   "source": [
    "### Page Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "9FfrJ0ZJDq0A"
   },
   "outputs": [],
   "source": [
    "def page_rank(domain):\n",
    "    import requests\n",
    "    key = '8o0gg0804g4k0gwkk4oocws04oc0sg88gg844o4k'\n",
    "    url = 'https://openpagerank.com/api/v1.0/getPageRank?domains%5B0%5D=' + str(domain)\n",
    "    try:\n",
    "        request = requests.get(url, headers={'API-OPR': key})\n",
    "        result = request.json()\n",
    "        result = result['response'][0]['page_rank_integer']\n",
    "        if result / 10 < 0.2 :\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    except:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "1jFn7h2TDtW6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(page_rank('http://postdebanks.com/DIE/POST/diepost/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHl_tpunEUv5"
   },
   "source": [
    "### Google Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "ZunJvIiF71Mj"
   },
   "outputs": [],
   "source": [
    "def google_index(url):\n",
    "    from urllib.parse import urlencode\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    #time.sleep(.6)\n",
    "    user_agent =  'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.116 Safari/537.36'\n",
    "    headers = {'User-Agent' : user_agent}\n",
    "    query = {'q': 'site:' + url}\n",
    "    google = \"https://www.google.com/search?\" + urlencode(query)\n",
    "    data = requests.get(google, headers=headers)\n",
    "    data.encoding = 'ISO-8859-1'\n",
    "    soup = BeautifulSoup(str(data.content), \"html.parser\")\n",
    "    try:\n",
    "      if 'Our systems have detected unusual traffic from your computer network.' in str(soup):\n",
    "        return -1\n",
    "      check = soup.find(id=\"rso\").find(\"div\").find(\"div\").find(\"a\")\n",
    "      #print(soup.prettify())\n",
    "      if check and check['href']:\n",
    "        return -1\n",
    "      else:\n",
    "        return 1\n",
    "        \n",
    "    except AttributeError:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "SVJtujPgE1nR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(google_index('http://walletconnectbits.com/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDMarF2_W-zH"
   },
   "source": [
    "### Links pointing to page (No of Hyperlinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "bVHhoiU5W-Ym"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import requests\n",
    "from requests.exceptions import RequestException\n",
    "\n",
    "def urlsCount(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an error for bad responses (4xx or 5xx)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        foundUrls = Counter([link[\"href\"] for link in soup.find_all(\"a\", href=lambda href: href and not href.startswith(\"#\"))])\n",
    "        count = len(foundUrls)\n",
    "        if count == 0:\n",
    "            return 1\n",
    "        elif 0 < count <= 2:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "    except RequestException as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return 1  # Return a default value or handle the error as needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "ZxoP_8gTXr9G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: HTTPSConnectionPool(host='www.hokabutypolska.pl', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002749BA7CFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "print(urlsCount('https://www.hokabutypolska.pl/'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhbqtHmnYbBo"
   },
   "source": [
    "### statistical report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "KehpaMN_YaT-"
   },
   "outputs": [],
   "source": [
    "def statistical_report(url, domain):\n",
    "    import re\n",
    "    import socket\n",
    "    url_match=re.search('at\\.ua|usa\\.cc|baltazarpresentes\\.com\\.br|pe\\.hu|esy\\.es|hol\\.es|sweddy\\.com|myjino\\.ru|96\\.lt|ow\\.ly',url)\n",
    "    try:\n",
    "        ip_address=socket.gethostbyname(domain)\n",
    "        ip_match=re.search('146\\.112\\.61\\.108|213\\.174\\.157\\.151|121\\.50\\.168\\.88|192\\.185\\.217\\.116|78\\.46\\.211\\.158|181\\.174\\.165\\.13|46\\.242\\.145\\.103|121\\.50\\.168\\.40|83\\.125\\.22\\.219|46\\.242\\.145\\.98|'\n",
    "                           '107\\.151\\.148\\.44|107\\.151\\.148\\.107|64\\.70\\.19\\.203|199\\.184\\.144\\.27|107\\.151\\.148\\.108|107\\.151\\.148\\.109|119\\.28\\.52\\.61|54\\.83\\.43\\.69|52\\.69\\.166\\.231|216\\.58\\.192\\.225|'\n",
    "                           '118\\.184\\.25\\.86|67\\.208\\.74\\.71|23\\.253\\.126\\.58|104\\.239\\.157\\.210|175\\.126\\.123\\.219|141\\.8\\.224\\.221|10\\.10\\.10\\.10|43\\.229\\.108\\.32|103\\.232\\.215\\.140|69\\.172\\.201\\.153|'\n",
    "                           '216\\.218\\.185\\.162|54\\.225\\.104\\.146|103\\.243\\.24\\.98|199\\.59\\.243\\.120|31\\.170\\.160\\.61|213\\.19\\.128\\.77|62\\.113\\.226\\.131|208\\.100\\.26\\.234|195\\.16\\.127\\.102|195\\.16\\.127\\.157|'\n",
    "                           '34\\.196\\.13\\.28|103\\.224\\.212\\.222|172\\.217\\.4\\.225|54\\.72\\.9\\.51|192\\.64\\.147\\.141|198\\.200\\.56\\.183|23\\.253\\.164\\.103|52\\.48\\.191\\.26|52\\.214\\.197\\.72|87\\.98\\.255\\.18|209\\.99\\.17\\.27|'\n",
    "                           '216\\.38\\.62\\.18|104\\.130\\.124\\.96|47\\.89\\.58\\.141|78\\.46\\.211\\.158|54\\.86\\.225\\.156|54\\.82\\.156\\.19|37\\.157\\.192\\.102|204\\.11\\.56\\.48|110\\.34\\.231\\.42',ip_address)\n",
    "        if url_match or ip_match:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    except:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "4ANKeJXpYcom"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(statistical_report('http://postdebanks.com/DIE/POST/diepost/', 'postdebanks.com'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7zExvXQL5TU"
   },
   "source": [
    "### Input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "tjkmmJ-DL9eQ"
   },
   "outputs": [],
   "source": [
    "def getInput(url):\n",
    "  from urllib.parse import urlparse\n",
    "  domain = urlparse(url).netloc\n",
    "  input = []\n",
    "  print(domain)\n",
    "  input.append(having_ip_address(url))\n",
    "  input.append(URL_Length(url))\n",
    "  input.append(haveAtSign(url))\n",
    "  input.append(prefixSuffix(url))\n",
    "  input.append(sub_domain_count(url))\n",
    "  input.append(sslVerify(url))\n",
    "  input.append(port(domain))\n",
    "  input.append(request_url(url))\n",
    "  input.append(url_anchor(url))\n",
    "  input.append(links_tag(url))\n",
    "  input.append(sfh(url))\n",
    "  input.append(sub_email(url))\n",
    "  input.append(mouse_over(url))\n",
    "  input.append(right_click(url))\n",
    "  input.append(domain_age(url))\n",
    "  input.append(dns_record(url))\n",
    "  input.append(web_traffic(url))\n",
    "  input.append(page_rank(domain))\n",
    "  input.append(google_index(url))\n",
    "  input.append(urlsCount(url))\n",
    "  input.append(statistical_report(url, domain))\n",
    "  return (input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-xdaTIIQYY6"
   },
   "outputs": [],
   "source": [
    "input = getInput('https://www.youtube.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G2zza5l7aL-w",
    "outputId": "019b934b-a1bf-4bda-a082-6cc7c63ea02d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 1, 1, -1, -1, -1, 1, 1, -1, 1, -1, -1]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jOo8d7PXaqs4",
    "outputId": "1ff7c2cf-bfa9-43ce-db99-8f249ef427f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 1, 1, -1, -1, -1, 1, 1, -1, 1, -1, -1]]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[input]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "L5ZTB0JmbhqR"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = r\"finalized_model.sav\"\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "fjwL-IeS6GHe"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DoM3UmnmaogS",
    "outputId": "a2995010-7806-4fae-d0d1-0112d2975839"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A legitimate website\n"
     ]
    }
   ],
   "source": [
    "result = loaded_model.predict([input])\n",
    "if result == -1:\n",
    "  print(\"A legitimate website\")\n",
    "else:\n",
    "  print(\"A Phishing website!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "feattures.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
